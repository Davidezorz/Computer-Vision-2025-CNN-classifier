import torch
import torch.nn as nn
import math
from . import parser


"""
╭ CONVENTIONS ────────────────────────────────────────────────────────────────╮
│ ├─• B        ▶ batch size                                                   │
│ ├─• H        ▶ height of the image                                          │
│ ├─• W        ▶ width of the image                                           │
│ ╰─• C        ▶ number of channerls                                          │
╰─────────────────────────────────────────────────────────────────────────────╯
"""


class CNN(nn.Module):
    """
    Dynamic Convolutional Neural Network builder.

    Constructs a sequential PyTorch model from a list of configuration 
    dictionaries. It manages the transition of tensor shapes (spatial -> flat)
    automatically, removing the need for manual dimension calculations between
    blocks.

    Args:
        image_dims (tuple): Input image dimensions. 
                            Supported formats: (H, W) or (C, H, W).
                            If (H, W) is provided, C=1 is assumed (grayscale).
        configs (list):     Ordered list of layer configurations. 
                            Typically generated by 'convParser', but can be 
                            built manually.
                            
                            Required keys per config dict:
                            - 'category': 'conv', 'linear', 'function', or 'skip'
                            - 'class':    The PyTorch class (e.g., nn.Conv2d)
                            - 'args':     Dict of arguments for that class
                            
                            Note: For manual skip connections, you must manage
                            SkipStore, SkipAdd, and SkipHandle classes yourself.

                            Note:  in_channels of Conv Layer and in_features in
                            linear layer, -1 means that the value is inferred

    Example:
        import torch.nn as nn
        configs = [
            {'category': 'conv',   'class': nn.Conv2d,                           
             'args': {'in_channels': -1, 'out_channels': 6, 'kernel_size': (5,5)
                      'padding': (2,2),  'stride': (1,1),  'dilation': (1,1)}},
            
            {'category': 'function', 'class': nn.ReLU, 'args': {}},

            {'category': 'linear', 'class': nn.Linear,   
             'args': {'in_features': -1, 'out_features': 10}}
        ]
        model = CNN(image_dims=(28, 28), configs=configs)
    """

    def __init__(self, image_dims, configs):
        super().__init__()
        blocks = []
        image_dims = list(image_dims)

        self.current_dims = image_dims if len(image_dims)>2 else [1]+image_dims 
        self.in_dims = self.current_dims.copy()
        print(self.current_dims)

        for config in configs:                                                  # For each config container
            self._inferredOutDim(config['args'], config['category'])            # ◀── Updates self.current_dims
            module = config['class'](**config['args'])                          # ◀─┬ Add the module cratedusing 
            blocks.append(module)                                               # ◀─┴ the correct arguments

            print(f"{config['type']}: \t {self.current_dims}")

        self.blocks    = nn.ModuleList(blocks)
        self.apply(self._initWeights)


    def _inferredOutDim(self, kwargs, category):
        """ Updates self.current_dims based on the layer type and
        infers missing input arguments (e.g., in_channels=-1)."""
        if category == 'conv':
            C, H, W = self.current_dims                                         # ◀── Unpack current shape
            if kwargs.get('in_channels') == -1:                                 #   ╭ Auto-fill input channels
                kwargs['in_channels'] = C                                       # ◀─┴ if not provided
            self.current_dims[0] = kwargs.get('out_channels', C)                # ◀─┬ Update C
            self.current_dims[1] = self._outDim([H, W], kwargs, 0)              # ◀─┼ Update H
            self.current_dims[2] = self._outDim([H, W], kwargs, 1)              # ◀─┴ Update W
            
        elif category == 'linear':                                              #   ╭ If we hit a Linear layer, the input 
            if kwargs.get('in_features') == -1:                                 # ◀─┼ is the flattened previous volume
                kwargs['in_features'] = math.prod(self.current_dims)            # ◀─┴ Calculates C*H*W
            self.current_dims = [kwargs['out_features']]                        # ◀── collapse everything into a 1D feature vector



    def _outDim(self, in_dim, kwargs, idx):
        """ Calculates output spatial dimension: 
        https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html """ 
        coef1 = in_dim[idx] + 2*kwargs['padding'][idx]
        coef2 = kwargs['dilation'][idx] * (kwargs['kernel_size'][idx]-1)
        coef3 = (coef1 - coef2 - 1)/kwargs['stride'][idx]

        return math.floor(coef3 + 1)


    def _initWeights(self, module):
        """Applies Kaiming initialization to Linear and Conv layers."""
        if isinstance(module, nn.Linear):                                       # Initialize Linear Layers
            nn.init.kaiming_uniform_(module.weight, mode='fan_in')
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)
                
        elif isinstance(module, nn.Conv2d):                                     # Initialize Convolutional Layers
            nn.init.kaiming_normal_(module.weight, mode='fan_out')
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)


    def forward(self, x):
        if x.ndim < 4:                                                          #   ╭ Handle single image input
            x = x.view(1, *self.in_dims)                                        # ◀─┴ Reshape -> 1 C H W

        for i, block in enumerate(self.blocks[:-1]):                            #   ╭ Apply all 
            x = block(x)                                                        # ◀─┴ Blocks
            # print(f"{i}: {x.shape}") 

        logits = self.blocks[-1](x)                                             #   ╭ compute and return the 
        return logits                                                           # ◀─┴ last layer (Logits)
    
    
    @torch.no_grad()
    def predict(self, x):
        self.eval()
        logits = self(x)
        probs = torch.softmax(logits, dim=-1)
        return probs.argmax(dim=-1)